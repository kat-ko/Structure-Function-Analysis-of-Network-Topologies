{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import networkx as nx\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import tempfile\n",
    "try:\n",
    "    import imageio.v2 as imageio\n",
    "except ImportError:\n",
    "    try:\n",
    "        import imageio\n",
    "    except ImportError:\n",
    "        imageio = None\n",
    "\n",
    "\n",
    "def create_small_world_topology(input_dim: int, hidden_size: int, output_dim: int, \n",
    "                                k: int, p: float, seed: int) -> Tuple[nx.DiGraph, List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate the small-world network topology as a single connected graph.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Number of input nodes\n",
    "        hidden_size: Number of hidden nodes\n",
    "        output_dim: Number of output nodes\n",
    "        k: Number of neighbors in ring lattice (must be < hidden_size)\n",
    "        p: Rewiring probability\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (topology graph, input_nodes list, output_nodes list)\n",
    "    \"\"\"\n",
    "    # Bounds checking\n",
    "    if k >= hidden_size:\n",
    "        raise ValueError(f\"k ({k}) must be less than hidden_size ({hidden_size})\")\n",
    "    if k < 2:\n",
    "        raise ValueError(f\"k ({k}) must be at least 2 for ring lattice\")\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    total_nodes = input_dim + hidden_size + output_dim\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(total_nodes))\n",
    "    \n",
    "    hidden_start = input_dim\n",
    "    hidden_end = hidden_start + hidden_size\n",
    "    \n",
    "    # Create initial ring lattice structure for hidden nodes (directed, acyclic)\n",
    "    for i in range(hidden_start, hidden_end):\n",
    "        # Only add edges to higher-indexed hidden nodes to maintain acyclicity\n",
    "        for j in range(1, k // 2 + 1):\n",
    "            target = hidden_start + ((i - hidden_start + j) % hidden_size)\n",
    "            if target > i and target < hidden_end:  # Only add forward edges within hidden layer\n",
    "                G.add_edge(i, target)\n",
    "    \n",
    "    # Rewire edges with probability p (maintaining acyclicity)\n",
    "    for edge in list(G.edges()):\n",
    "        if rng.random() < p:\n",
    "            # Remove the edge\n",
    "            G.remove_edge(*edge)\n",
    "            # Add a new random edge (only to higher-indexed hidden nodes)\n",
    "            # Error handling: ensure we can find a valid target\n",
    "            if edge[0] + 1 < hidden_end:\n",
    "                max_attempts = 100\n",
    "                attempts = 0\n",
    "                new_node = rng.randint(edge[0] + 1, hidden_end)\n",
    "                while G.has_edge(edge[0], new_node) and attempts < max_attempts:\n",
    "                    new_node = rng.randint(edge[0] + 1, hidden_end)\n",
    "                    attempts += 1\n",
    "                if attempts < max_attempts:\n",
    "                    G.add_edge(edge[0], new_node)\n",
    "                # If we can't find a valid edge, just skip rewiring for this edge\n",
    "    \n",
    "    # Add connections from input nodes to hidden nodes\n",
    "    if input_dim is not None:\n",
    "        for input_node in range(input_dim):\n",
    "            for hidden_node in range(hidden_start, hidden_start + min(k, hidden_size)):\n",
    "                G.add_edge(input_node, hidden_node)\n",
    "    \n",
    "    # Add connections from hidden nodes to output nodes\n",
    "    if output_dim is not None:\n",
    "        for output_node in range(hidden_end, total_nodes):\n",
    "            for hidden_node in range(hidden_end - min(k, hidden_size), hidden_end):\n",
    "                G.add_edge(hidden_node, output_node)\n",
    "    \n",
    "    # Ensure topology is a DAG\n",
    "    if not nx.is_directed_acyclic_graph(G):\n",
    "        raise ValueError(\"FFN requires a Directed Acyclic Graph (DAG) topology\")\n",
    "    \n",
    "    input_nodes = list(range(input_dim))\n",
    "    output_nodes = list(range(input_dim + hidden_size, input_dim + hidden_size + output_dim))\n",
    "    \n",
    "    return G, input_nodes, output_nodes\n",
    "\n",
    "\n",
    "class TopologyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch module for a feedforward network with custom topology.\n",
    "    Maintains interpretability through manual topological forward pass while\n",
    "    using PyTorch's autograd for efficient training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, topology: nx.DiGraph, input_nodes: List[int], \n",
    "                 output_nodes: List[int], seed: int = 47, activation: str = 'leaky_relu'):\n",
    "        \"\"\"\n",
    "        Initialize the topology network.\n",
    "        \n",
    "        Args:\n",
    "            topology: NetworkX DiGraph representing the network topology\n",
    "            input_nodes: List of input node indices\n",
    "            output_nodes: List of output node indices\n",
    "            seed: Random seed for weight initialization\n",
    "            activation: Activation function ('leaky_relu' or 'tanh')\n",
    "        \"\"\"\n",
    "        super(TopologyNetwork, self).__init__()\n",
    "        \n",
    "        self.topology = topology\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.node_order = list(nx.topological_sort(topology))\n",
    "        \n",
    "        # Set seed for reproducible weight initialization\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Convert node_states to nn.Parameters for automatic gradient computation\n",
    "        self.biases = nn.ParameterDict()\n",
    "        self.weights = nn.ParameterDict()\n",
    "\n",
    "        for node in list(topology.nodes()):\n",
    "            # Initialize bias as a parameter\n",
    "            bias_value = np.random.normal(0, 0.1)\n",
    "            self.biases[str(node)] = nn.Parameter(torch.tensor(bias_value, dtype=torch.float32))\n",
    "            \n",
    "            # Initialize weights for incoming edges (predecessors)\n",
    "            for neighbor in topology.predecessors(node):\n",
    "                weight_value = np.random.normal(0, 0.1)\n",
    "                param_name = f\"{neighbor}_to_{node}\"\n",
    "                self.weights[param_name] = nn.Parameter(torch.tensor(weight_value, dtype=torch.float32))\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(0.1)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the topology network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape [batch_size, output_dim] (Q-values)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "        \n",
    "        # Initialize activations dictionary\n",
    "        activations = {}\n",
    "        \n",
    "        # Set input node activations\n",
    "        for i, input_node in enumerate(self.input_nodes):\n",
    "            activations[input_node] = x[:, i]\n",
    "        \n",
    "        # Initialize all other nodes to zero\n",
    "        for node in self.topology.nodes():\n",
    "            if node not in activations:\n",
    "                activations[node] = torch.zeros(batch_size, device=device)\n",
    "        \n",
    "        # Process through network in topological order\n",
    "        for node in self.node_order:\n",
    "            if node not in self.input_nodes:\n",
    "                # Get bias\n",
    "                bias = self.biases[str(node)]\n",
    "                \n",
    "                # Sum weighted inputs from predecessors\n",
    "                weighted_sum = torch.full((batch_size,), bias.item(), dtype=torch.float32, device=device)\n",
    "                \n",
    "                for neighbor in self.topology.predecessors(node):\n",
    "                    weight_param = self.weights[f\"{neighbor}_to_{node}\"]\n",
    "                    weighted_sum = weighted_sum + activations[neighbor] * weight_param\n",
    "                \n",
    "                # Apply activation function\n",
    "                activations[node] = self.activation(weighted_sum)\n",
    "        \n",
    "        # Collect output node activations\n",
    "        output_values = []\n",
    "        for output_node in sorted(self.output_nodes):\n",
    "            output_values.append(activations[output_node])\n",
    "        \n",
    "        # Stack into [batch_size, output_dim] tensor\n",
    "        return torch.stack(output_values, dim=1)\n",
    "    \n",
    "    def get_weights_dict(self) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"\n",
    "        Extract all edge weights as dictionary mapping (source, target) to weight value.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping (source_node, target_node) tuples to weight values\n",
    "        \"\"\"\n",
    "        weights_dict = {}\n",
    "        for param_name, param in self.weights.items():\n",
    "            # Parse param_name format: \"{source}_to_{target}\"\n",
    "            parts = param_name.split('_to_')\n",
    "            if len(parts) == 2:\n",
    "                source = int(parts[0])\n",
    "                target = int(parts[1])\n",
    "                weights_dict[(source, target)] = param.data.cpu().item()\n",
    "        return weights_dict\n",
    "    \n",
    "    def get_biases_dict(self) -> Dict[int, float]:\n",
    "        \"\"\"\n",
    "        Extract all node biases as dictionary mapping node index to bias value.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping node indices to bias values\n",
    "        \"\"\"\n",
    "        biases_dict = {}\n",
    "        for node_str, param in self.biases.items():\n",
    "            node = int(node_str)\n",
    "            biases_dict[node] = param.data.cpu().item()\n",
    "        return biases_dict\n",
    "\n",
    "\n",
    "class ExperienceReplayBuffer:\n",
    "    \"\"\"\n",
    "    Simple experience replay buffer for DQN training.\n",
    "    Stores transitions and provides random batch sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum number of transitions to store\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def store(self, state: np.ndarray, action: int, reward: float, \n",
    "              next_state: np.ndarray, done: bool):\n",
    "        \"\"\"\n",
    "        Store a transition in the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode is done\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, \n",
    "                                               torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample a random batch of transitions.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of transitions to sample\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, rewards, next_states, dones) as tensors\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Convert to numpy arrays first for better performance, then to tensors\n",
    "        states = torch.FloatTensor(np.array([e[0] for e in batch]))\n",
    "        actions = torch.LongTensor(np.array([e[1] for e in batch]))\n",
    "        rewards = torch.FloatTensor(np.array([e[2] for e in batch]))\n",
    "        next_states = torch.FloatTensor(np.array([e[3] for e in batch]))\n",
    "        dones = torch.BoolTensor(np.array([e[4] for e in batch]))\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN agent that uses a custom topology network for Q-value estimation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, network: TopologyNetwork, learning_rate: float = 0.001, \n",
    "                 gamma: float = 0.99, device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "        \n",
    "        Args:\n",
    "            network: TopologyNetwork instance for the main network\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            gamma: Discount factor\n",
    "            device: Device to run on ('cpu' or 'cuda')\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Main network\n",
    "        self.main_network = network.to(self.device)\n",
    "        \n",
    "        # Target network (copy of main network)\n",
    "        self.target_network = TopologyNetwork(\n",
    "            network.topology, network.input_nodes, network.output_nodes,\n",
    "            seed=47, activation='leaky_relu'\n",
    "        ).to(self.device)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.main_network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, epsilon: float) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            epsilon: Exploration probability\n",
    "            \n",
    "        Returns:\n",
    "            Selected action\n",
    "        \"\"\"\n",
    "        num_actions = len(self.main_network.output_nodes)\n",
    "        if random.random() < epsilon:\n",
    "            # Random action\n",
    "            return random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            # Greedy action\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.main_network(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def train_step(self, replay_buffer: ExperienceReplayBuffer, batch_size: int) -> float:\n",
    "        \"\"\"\n",
    "        Perform one training step.\n",
    "        \n",
    "        Args:\n",
    "            replay_buffer: Experience replay buffer\n",
    "            batch_size: Batch size for training\n",
    "            \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Compute current Q-values: Q(s, a)\n",
    "        current_q_values = self.main_network(states)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values: r + Î³ * max(Q_target(s', a')) * (1 - done)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * max_next_q_values * (~dones).float()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy main network parameters to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "    \n",
    "    def get_current_weights_biases(self) -> Tuple[Dict[Tuple[int, int], float], Dict[int, float]]:\n",
    "        \"\"\"\n",
    "        Get current weights and biases from the main network.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (weights_dict, biases_dict)\n",
    "        \"\"\"\n",
    "        return self.main_network.get_weights_dict(), self.main_network.get_biases_dict()\n",
    "\n",
    "\n",
    "def visualize_network_topology(topology: nx.DiGraph, input_nodes: List[int], \n",
    "                                output_nodes: List[int], weights: Dict[Tuple[int, int], float],\n",
    "                                biases: Dict[int, float], weight_range: Optional[Tuple[float, float]] = None,\n",
    "                                bias_range: Optional[Tuple[float, float]] = None,\n",
    "                                figsize: Tuple[int, int] = (16, 10)) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualize network topology with colored nodes (by bias) and edges (by weight).\n",
    "    \n",
    "    Args:\n",
    "        topology: NetworkX DiGraph representing the topology\n",
    "        input_nodes: List of input node indices\n",
    "        output_nodes: List of output node indices\n",
    "        weights: Dictionary mapping (source, target) to weight values\n",
    "        biases: Dictionary mapping node index to bias values\n",
    "        weight_range: Optional (min, max) for weight colormap normalization\n",
    "        bias_range: Optional (min, max) for bias colormap normalization\n",
    "        figsize: Figure size tuple\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Separate nodes\n",
    "    hidden_nodes = [n for n in topology.nodes() \n",
    "                    if n not in input_nodes and n not in output_nodes]\n",
    "    \n",
    "    # Determine ranges for colormap\n",
    "    if weight_range is None:\n",
    "        weight_values = list(weights.values()) if weights else [0.0]\n",
    "        weight_min, weight_max = min(weight_values), max(weight_values)\n",
    "        weight_abs_max = max(abs(weight_min), abs(weight_max))\n",
    "        weight_range = (-weight_abs_max, weight_abs_max) if weight_abs_max > 0 else (-1.0, 1.0)\n",
    "    \n",
    "    if bias_range is None:\n",
    "        bias_values = list(biases.values()) if biases else [0.0]\n",
    "        bias_min, bias_max = min(bias_values), max(bias_values)\n",
    "        bias_abs_max = max(abs(bias_min), abs(bias_max))\n",
    "        bias_range = (-bias_abs_max, bias_abs_max) if bias_abs_max > 0 else (-1.0, 1.0)\n",
    "    \n",
    "    # Create custom colormap (red for positive, blue for negative, white for zero)\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors = ['#0000FF', '#FFFFFF', '#FF0000']  # Blue, White, Red\n",
    "    n_bins = 256\n",
    "    cmap = LinearSegmentedColormap.from_list('diverging', colors, N=n_bins)\n",
    "    \n",
    "    # Define column positions\n",
    "    col_x = {'input': 0, 'hidden': 1, 'output': 2}\n",
    "    col_spacing = 3.0\n",
    "    \n",
    "    # Calculate node positions\n",
    "    pos = {}\n",
    "    node_y_spacing = 0.8\n",
    "    \n",
    "    # Input nodes (left column)\n",
    "    for i, node in enumerate(sorted(input_nodes)):\n",
    "        pos[node] = (col_x['input'] * col_spacing, \n",
    "                     (len(input_nodes) - 1 - i) * node_y_spacing - (len(input_nodes) - 1) * node_y_spacing / 2)\n",
    "    \n",
    "    # Hidden nodes (center column)\n",
    "    for i, node in enumerate(sorted(hidden_nodes)):\n",
    "        pos[node] = (col_x['hidden'] * col_spacing,\n",
    "                     (len(hidden_nodes) - 1 - i) * node_y_spacing - (len(hidden_nodes) - 1) * node_y_spacing / 2)\n",
    "    \n",
    "    # Output nodes (right column)\n",
    "    for i, node in enumerate(sorted(output_nodes)):\n",
    "        pos[node] = (col_x['output'] * col_spacing,\n",
    "                       (len(output_nodes) - 1 - i) * node_y_spacing - (len(output_nodes) - 1) * node_y_spacing / 2)\n",
    "    \n",
    "    # Draw edges with colors based on weights\n",
    "    for edge in topology.edges():\n",
    "        source, target = edge\n",
    "        if (source, target) in weights:\n",
    "            weight_val = weights[(source, target)]\n",
    "            # Normalize weight to [0, 1] for colormap\n",
    "            weight_norm = (weight_val - weight_range[0]) / (weight_range[1] - weight_range[0])\n",
    "            weight_norm = np.clip(weight_norm, 0, 1)\n",
    "            edge_color = cmap(weight_norm)\n",
    "        else:\n",
    "            edge_color = 'gray'\n",
    "        \n",
    "        # Draw edge with arrow\n",
    "        x_coords = [pos[source][0], pos[target][0]]\n",
    "        y_coords = [pos[source][1], pos[target][1]]\n",
    "        ax.plot(x_coords, y_coords, color=edge_color, alpha=0.6, linewidth=1.5, zorder=1)\n",
    "        \n",
    "        # Draw arrowhead\n",
    "        dx = x_coords[1] - x_coords[0]\n",
    "        dy = y_coords[1] - y_coords[0]\n",
    "        length = np.sqrt(dx**2 + dy**2)\n",
    "        if length > 0:\n",
    "            arrow_length = 0.15\n",
    "            arrow_dx = dx / length * arrow_length\n",
    "            arrow_dy = dy / length * arrow_length\n",
    "            ax.arrow(x_coords[1] - arrow_dx, y_coords[1] - arrow_dy,\n",
    "                    arrow_dx, arrow_dy, head_width=0.1, head_length=0.08,\n",
    "                    fc=edge_color, ec=edge_color, alpha=0.6, zorder=2)\n",
    "    \n",
    "    # Draw nodes with colors based on biases\n",
    "    for node in topology.nodes():\n",
    "        if node in biases:\n",
    "            bias_val = biases[node]\n",
    "            # Normalize bias to [0, 1] for colormap\n",
    "            bias_norm = (bias_val - bias_range[0]) / (bias_range[1] - bias_range[0])\n",
    "            bias_norm = np.clip(bias_norm, 0, 1)\n",
    "            node_color = cmap(bias_norm)\n",
    "        else:\n",
    "            node_color = 'gray'\n",
    "        \n",
    "        node_size = 300 if node in input_nodes or node in output_nodes else 200\n",
    "        ax.scatter(pos[node][0], pos[node][1], s=node_size, c=[node_color], \n",
    "                  edgecolors='black', linewidths=1.5, zorder=3)\n",
    "        ax.text(pos[node][0], pos[node][1], str(node), ha='center', va='center',\n",
    "               fontsize=8, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Add colorbar for weights\n",
    "    sm_weight = plt.cm.ScalarMappable(cmap=cmap, \n",
    "                                      norm=plt.Normalize(vmin=weight_range[0], vmax=weight_range[1]))\n",
    "    sm_weight.set_array([])\n",
    "    cbar_weight = plt.colorbar(sm_weight, ax=ax, label='Edge Weight', pad=0.02, shrink=0.6)\n",
    "    cbar_weight.set_label('Edge Weight', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add colorbar for biases\n",
    "    sm_bias = plt.cm.ScalarMappable(cmap=cmap,\n",
    "                                    norm=plt.Normalize(vmin=bias_range[0], vmax=bias_range[1]))\n",
    "    sm_bias.set_array([])\n",
    "    cbar_bias = plt.colorbar(sm_bias, ax=ax, label='Node Bias', pad=0.02, shrink=0.6, location='right')\n",
    "    cbar_bias.set_label('Node Bias', rotation=270, labelpad=20)\n",
    "    \n",
    "    ax.set_xlim(-1, col_spacing * 2 + 1)\n",
    "    ax.set_ylim(-max(len(input_nodes), len(hidden_nodes), len(output_nodes)) * node_y_spacing / 2 - 1,\n",
    "                max(len(input_nodes), len(hidden_nodes), len(output_nodes)) * node_y_spacing / 2 + 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Network Topology (Nodes: Bias, Edges: Weight)', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_reward_curve(episode_rewards: List[float], current_episode: int,\n",
    "                      figsize: Tuple[int, int] = (12, 6)) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot reward curve up to current episode.\n",
    "    \n",
    "    Args:\n",
    "        episode_rewards: List of episode rewards\n",
    "        current_episode: Current episode number (to plot up to)\n",
    "        figsize: Figure size tuple\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    episodes_to_plot = min(current_episode, len(episode_rewards))\n",
    "    if episodes_to_plot > 0:\n",
    "        episodes = list(range(1, episodes_to_plot + 1))\n",
    "        rewards = episode_rewards[:episodes_to_plot]\n",
    "        \n",
    "        ax.plot(episodes, rewards, 'b-', alpha=0.6, linewidth=1, label='Episode Reward')\n",
    "        \n",
    "        # Calculate and plot moving average\n",
    "        if len(rewards) >= 10:\n",
    "            window = min(50, len(rewards))\n",
    "            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            moving_episodes = list(range(window, episodes_to_plot + 1))\n",
    "            ax.plot(moving_episodes, moving_avg, 'r-', linewidth=2, label=f'Moving Avg ({window} episodes)')\n",
    "    \n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_ylabel('Average Reward', fontsize=12)\n",
    "    ax.set_title('Training Progress: Reward Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_training_frame(topology: nx.DiGraph, input_nodes: List[int], output_nodes: List[int],\n",
    "                          weights: Dict[Tuple[int, int], float], biases: Dict[int, float],\n",
    "                          episode_rewards: List[float], current_episode: int,\n",
    "                          weight_range: Optional[Tuple[float, float]] = None,\n",
    "                          bias_range: Optional[Tuple[float, float]] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create a combined frame with network topology (top) and reward curve (bottom).\n",
    "    \n",
    "    Args:\n",
    "        topology: NetworkX DiGraph representing the topology\n",
    "        input_nodes: List of input node indices\n",
    "        output_nodes: List of output node indices\n",
    "        weights: Dictionary mapping (source, target) to weight values\n",
    "        biases: Dictionary mapping node index to bias values\n",
    "        episode_rewards: List of episode rewards\n",
    "        current_episode: Current episode number\n",
    "        weight_range: Optional (min, max) for weight colormap normalization\n",
    "        bias_range: Optional (min, max) for bias colormap normalization\n",
    "        \n",
    "    Returns:\n",
    "        Combined matplotlib figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Create a grid layout with more control\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=[2, 1], hspace=0.3)\n",
    "    \n",
    "    # Top subplot: Network topology (takes more space)\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    \n",
    "    # Use the same visualization logic but on the subplot\n",
    "    hidden_nodes = [n for n in topology.nodes() \n",
    "                    if n not in input_nodes and n not in output_nodes]\n",
    "    \n",
    "    # Determine ranges\n",
    "    if weight_range is None:\n",
    "        weight_values = list(weights.values()) if weights else [0.0]\n",
    "        weight_min, weight_max = min(weight_values), max(weight_values)\n",
    "        weight_abs_max = max(abs(weight_min), abs(weight_max))\n",
    "        weight_range = (-weight_abs_max, weight_abs_max) if weight_abs_max > 0 else (-1.0, 1.0)\n",
    "    \n",
    "    if bias_range is None:\n",
    "        bias_values = list(biases.values()) if biases else [0.0]\n",
    "        bias_min, bias_max = min(bias_values), max(bias_values)\n",
    "        bias_abs_max = max(abs(bias_min), abs(bias_max))\n",
    "        bias_range = (-bias_abs_max, bias_abs_max) if bias_abs_max > 0 else (-1.0, 1.0)\n",
    "    \n",
    "    # Create colormap\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors = ['#0000FF', '#FFFFFF', '#FF0000']\n",
    "    n_bins = 256\n",
    "    cmap = LinearSegmentedColormap.from_list('diverging', colors, N=n_bins)\n",
    "    \n",
    "    # Define positions - spread across more of the available width\n",
    "    # Use wider spacing to fill the available space better\n",
    "    col_x = {'input': 0, 'hidden': 1, 'output': 2}\n",
    "    col_spacing = 4.0  # Increased spacing to spread network wider\n",
    "    pos = {}\n",
    "    node_y_spacing = 0.8\n",
    "    \n",
    "    # Input nodes\n",
    "    for i, node in enumerate(sorted(input_nodes)):\n",
    "        pos[node] = (col_x['input'] * col_spacing,\n",
    "                     (len(input_nodes) - 1 - i) * node_y_spacing - (len(input_nodes) - 1) * node_y_spacing / 2)\n",
    "    \n",
    "    # Hidden nodes\n",
    "    for i, node in enumerate(sorted(hidden_nodes)):\n",
    "        pos[node] = (col_x['hidden'] * col_spacing,\n",
    "                     (len(hidden_nodes) - 1 - i) * node_y_spacing - (len(hidden_nodes) - 1) * node_y_spacing / 2)\n",
    "    \n",
    "    # Output nodes\n",
    "    for i, node in enumerate(sorted(output_nodes)):\n",
    "        pos[node] = (col_x['output'] * col_spacing,\n",
    "                     (len(output_nodes) - 1 - i) * node_y_spacing - (len(output_nodes) - 1) * node_y_spacing / 2)\n",
    "    \n",
    "    # Draw edges\n",
    "    for edge in topology.edges():\n",
    "        source, target = edge\n",
    "        if (source, target) in weights:\n",
    "            weight_val = weights[(source, target)]\n",
    "            weight_norm = (weight_val - weight_range[0]) / (weight_range[1] - weight_range[0])\n",
    "            weight_norm = np.clip(weight_norm, 0, 1)\n",
    "            edge_color = cmap(weight_norm)\n",
    "        else:\n",
    "            edge_color = 'gray'\n",
    "        \n",
    "        x_coords = [pos[source][0], pos[target][0]]\n",
    "        y_coords = [pos[source][1], pos[target][1]]\n",
    "        ax1.plot(x_coords, y_coords, color=edge_color, alpha=0.6, linewidth=1.5, zorder=1)\n",
    "        \n",
    "        dx = x_coords[1] - x_coords[0]\n",
    "        dy = y_coords[1] - y_coords[0]\n",
    "        length = np.sqrt(dx**2 + dy**2)\n",
    "        if length > 0:\n",
    "            arrow_length = 0.15\n",
    "            arrow_dx = dx / length * arrow_length\n",
    "            arrow_dy = dy / length * arrow_length\n",
    "            ax1.arrow(x_coords[1] - arrow_dx, y_coords[1] - arrow_dy,\n",
    "                     arrow_dx, arrow_dy, head_width=0.1, head_length=0.08,\n",
    "                     fc=edge_color, ec=edge_color, alpha=0.6, zorder=2)\n",
    "    \n",
    "    # Draw nodes\n",
    "    for node in topology.nodes():\n",
    "        if node in biases:\n",
    "            bias_val = biases[node]\n",
    "            bias_norm = (bias_val - bias_range[0]) / (bias_range[1] - bias_range[0])\n",
    "            bias_norm = np.clip(bias_norm, 0, 1)\n",
    "            node_color = cmap(bias_norm)\n",
    "        else:\n",
    "            node_color = 'gray'\n",
    "        \n",
    "        node_size = 300 if node in input_nodes or node in output_nodes else 200\n",
    "        ax1.scatter(pos[node][0], pos[node][1], s=node_size, c=[node_color],\n",
    "                   edgecolors='black', linewidths=1.5, zorder=3)\n",
    "        ax1.text(pos[node][0], pos[node][1], str(node), ha='center', va='center',\n",
    "                fontsize=8, fontweight='bold', zorder=4)\n",
    "    \n",
    "    # Set axis limits before adding colorbars to ensure consistent layout\n",
    "    max_height = max(len(input_nodes), len(hidden_nodes), len(output_nodes))\n",
    "    # Expand xlim to use more of the available space\n",
    "    ax1.set_xlim(-1.5, col_spacing * 2 + 1.5)\n",
    "    ax1.set_ylim(-max_height * node_y_spacing / 2 - 1,\n",
    "                max_height * node_y_spacing / 2 + 1)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(f'Network Topology - Episode {current_episode}', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add colorbars with better positioning - use divider to make room\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    \n",
    "    # Add weight colorbar on the right (smaller to take less space)\n",
    "    cax_weight = divider.append_axes(\"right\", size=\"2.5%\", pad=0.05)\n",
    "    sm_weight = plt.cm.ScalarMappable(cmap=cmap,\n",
    "                                      norm=plt.Normalize(vmin=weight_range[0], vmax=weight_range[1]))\n",
    "    sm_weight.set_array([])\n",
    "    cbar_weight = plt.colorbar(sm_weight, cax=cax_weight)\n",
    "    cbar_weight.set_label('Edge Weight', rotation=270, labelpad=12, fontsize=9)\n",
    "    \n",
    "    # Add bias colorbar further to the right\n",
    "    cax_bias = divider.append_axes(\"right\", size=\"2.5%\", pad=0.3)\n",
    "    sm_bias = plt.cm.ScalarMappable(cmap=cmap,\n",
    "                                    norm=plt.Normalize(vmin=bias_range[0], vmax=bias_range[1]))\n",
    "    sm_bias.set_array([])\n",
    "    cbar_bias = plt.colorbar(sm_bias, cax=cax_bias)\n",
    "    cbar_bias.set_label('Node Bias', rotation=270, labelpad=12, fontsize=9)\n",
    "    \n",
    "    # Bottom subplot: Reward curve\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    episodes_to_plot = min(current_episode, len(episode_rewards))\n",
    "    if episodes_to_plot > 0:\n",
    "        episodes = list(range(1, episodes_to_plot + 1))\n",
    "        rewards = episode_rewards[:episodes_to_plot]\n",
    "        \n",
    "        ax2.plot(episodes, rewards, 'b-', alpha=0.6, linewidth=1, label='Episode Reward')\n",
    "        \n",
    "        if len(rewards) >= 10:\n",
    "            window = min(50, len(rewards))\n",
    "            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            moving_episodes = list(range(window, episodes_to_plot + 1))\n",
    "            ax2.plot(moving_episodes, moving_avg, 'r-', linewidth=2, label=f'Moving Avg ({window} episodes)')\n",
    "    \n",
    "    ax2.set_xlabel('Episode', fontsize=12)\n",
    "    ax2.set_ylabel('Average Reward', fontsize=12)\n",
    "    ax2.set_title('Training Progress: Reward Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_training_video(history: Dict, output_path: str = \"training_evolution.mp4\",\n",
    "                         fps: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Create a video from training history showing network evolution and reward curve.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history dictionary with weight_history, bias_history, etc.\n",
    "        output_path: Path to save the video\n",
    "        fps: Frames per second for the video\n",
    "        \n",
    "    Returns:\n",
    "        Path to the created video file\n",
    "    \"\"\"\n",
    "    if imageio is None:\n",
    "        raise ImportError(\"imageio is required for video creation. Install it with: pip install imageio imageio-ffmpeg\")\n",
    "    \n",
    "    weight_history = history.get('weight_history', [])\n",
    "    bias_history = history.get('bias_history', [])\n",
    "    tracking_episodes = history.get('tracking_episodes', [])\n",
    "    episode_rewards = history.get('episode_rewards', [])\n",
    "    topology = history.get('topology')\n",
    "    input_nodes = history.get('input_nodes', [])\n",
    "    output_nodes = history.get('output_nodes', [])\n",
    "    \n",
    "    if not weight_history or not bias_history:\n",
    "        raise ValueError(\"No weight/bias history found in training history\")\n",
    "    \n",
    "    # Calculate global ranges for consistent colorbar\n",
    "    all_weights = []\n",
    "    all_biases = []\n",
    "    for weights in weight_history:\n",
    "        all_weights.extend(weights.values())\n",
    "    for biases in bias_history:\n",
    "        all_biases.extend(biases.values())\n",
    "    \n",
    "    weight_min, weight_max = min(all_weights), max(all_weights)\n",
    "    weight_abs_max = max(abs(weight_min), abs(weight_max))\n",
    "    weight_range = (-weight_abs_max, weight_abs_max) if weight_abs_max > 0 else (-1.0, 1.0)\n",
    "    \n",
    "    bias_min, bias_max = min(all_biases), max(all_biases)\n",
    "    bias_abs_max = max(abs(bias_min), abs(bias_max))\n",
    "    bias_range = (-bias_abs_max, bias_abs_max) if bias_abs_max > 0 else (-1.0, 1.0)\n",
    "    \n",
    "    # Create temporary directory for frames\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        frame_paths = []\n",
    "        \n",
    "        print(f\"Generating {len(weight_history)} frames...\")\n",
    "        for i, (weights, biases, episode) in enumerate(zip(weight_history, bias_history, tracking_episodes)):\n",
    "            # Create frame\n",
    "            fig = create_training_frame(topology, input_nodes, output_nodes,\n",
    "                                      weights, biases, episode_rewards, episode,\n",
    "                                      weight_range=weight_range, bias_range=bias_range)\n",
    "            \n",
    "            # Save frame with fixed size to ensure all frames are the same size\n",
    "            frame_path = os.path.join(temp_dir, f\"frame_{i:05d}.png\")\n",
    "            # Use fixed bbox and DPI to ensure consistent frame sizes\n",
    "            fig.savefig(frame_path, dpi=150, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "            plt.close(fig)\n",
    "            frame_paths.append(frame_path)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Generated {i + 1}/{len(weight_history)} frames\")\n",
    "        \n",
    "        # Create video from frames\n",
    "        print(f\"Creating video: {output_path}\")\n",
    "        \n",
    "        # Read first frame to get target size\n",
    "        first_frame = imageio.imread(frame_paths[0])\n",
    "        target_height, target_width = first_frame.shape[:2]\n",
    "        \n",
    "        # Ensure all frames have the same size\n",
    "        processed_frames = []\n",
    "        for i, frame_path in enumerate(frame_paths):\n",
    "            frame = imageio.imread(frame_path)\n",
    "            current_height, current_width = frame.shape[:2]\n",
    "            \n",
    "            # Resize if needed (shouldn't be needed with fixed bbox, but just in case)\n",
    "            if current_height != target_height or current_width != target_width:\n",
    "                from PIL import Image\n",
    "                img = Image.fromarray(frame)\n",
    "                img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)\n",
    "                frame = np.array(img)\n",
    "            \n",
    "            processed_frames.append(frame)\n",
    "        \n",
    "        # Write video with consistent frame sizes\n",
    "        with imageio.get_writer(output_path, fps=fps, macro_block_size=1) as writer:\n",
    "            for frame in processed_frames:\n",
    "                writer.append_data(frame)\n",
    "        \n",
    "        print(f\"Video saved to: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def train_dqn(env_name: str = \"CartPole-v1\", hidden_size: int = 64, k: int = 8, \n",
    "              p: float = 0.1, seed: int = 47, num_episodes: int = 500,\n",
    "              max_steps_per_episode: int = 500, batch_size: int = 32,\n",
    "              learning_rate: float = 0.001, gamma: float = 0.99,\n",
    "              epsilon_start: float = 1.0, epsilon_end: float = 0.01,\n",
    "              epsilon_decay: float = 0.995, target_update_freq: int = 10,\n",
    "              replay_buffer_size: int = 10000, min_buffer_size: int = 1000,\n",
    "              device: str = 'cpu', track_freq: int = 10):\n",
    "    \"\"\"\n",
    "    Train a DQN agent with custom topology network.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium environment name\n",
    "        hidden_size: Number of hidden nodes\n",
    "        k: Number of neighbors in ring lattice\n",
    "        p: Rewiring probability\n",
    "        seed: Random seed\n",
    "        num_episodes: Number of training episodes\n",
    "        max_steps_per_episode: Maximum steps per episode\n",
    "        batch_size: Batch size for training\n",
    "        learning_rate: Learning rate\n",
    "        gamma: Discount factor\n",
    "        epsilon_start: Initial exploration probability\n",
    "        epsilon_end: Final exploration probability\n",
    "        epsilon_decay: Epsilon decay factor per episode\n",
    "        target_update_freq: Frequency of target network updates (in episodes)\n",
    "        replay_buffer_size: Size of replay buffer\n",
    "        min_buffer_size: Minimum buffer size before training starts\n",
    "        device: Device to run on\n",
    "        track_freq: Frequency (in episodes) to capture weights/biases for visualization\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    # Set seeds\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    \n",
    "    # Get dimensions\n",
    "    from stable_baselines3.common.preprocessing import get_flattened_obs_dim\n",
    "    input_dim = int(get_flattened_obs_dim(env.observation_space))\n",
    "    action_space = env.action_space\n",
    "    if hasattr(action_space, 'n'):\n",
    "        output_dim = int(action_space.n)\n",
    "    else:\n",
    "        raise ValueError(\"Action space not properly configured\")\n",
    "    \n",
    "    # Create topology\n",
    "    topology, input_nodes, output_nodes = create_small_world_topology(\n",
    "        input_dim, hidden_size, output_dim, k, p, seed\n",
    "    )\n",
    "    \n",
    "    # Create network\n",
    "    network = TopologyNetwork(topology, input_nodes, output_nodes, seed=seed)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(network, learning_rate=learning_rate, gamma=gamma, device=device)\n",
    "    \n",
    "    # Create replay buffer\n",
    "    replay_buffer = ExperienceReplayBuffer(capacity=replay_buffer_size)\n",
    "    \n",
    "    # Training history\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    # Weight/bias tracking for visualization\n",
    "    weight_history = []\n",
    "    bias_history = []\n",
    "    tracking_episodes = []\n",
    "    \n",
    "    print(f\"Starting DQN training on {env_name}\")\n",
    "    print(f\"Network topology: {len(topology.nodes())} nodes, {len(topology.edges())} edges\")\n",
    "    print(f\"Input dim: {input_dim}, Output dim: {output_dim}, Hidden size: {hidden_size}\")\n",
    "    print(f\"Tracking weights/biases every {track_freq} episodes\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        num_train_steps = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Select action\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            replay_buffer.store(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train if buffer is large enough\n",
    "            if len(replay_buffer) >= min_buffer_size:\n",
    "                loss = agent.train_step(replay_buffer, batch_size)\n",
    "                episode_loss += loss\n",
    "                num_train_steps += 1\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Record history\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if num_train_steps > 0:\n",
    "            episode_losses.append(episode_loss / num_train_steps)\n",
    "        else:\n",
    "            episode_losses.append(0.0)\n",
    "        \n",
    "        # Track weights/biases at regular intervals\n",
    "        if (episode + 1) % track_freq == 0 or episode == 0:\n",
    "            weights, biases = agent.get_current_weights_biases()\n",
    "            weight_history.append(weights.copy())\n",
    "            bias_history.append(biases.copy())\n",
    "            tracking_episodes.append(episode + 1)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            avg_loss = np.mean(episode_losses[-50:]) if episode_losses else 0.0\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Epsilon: {epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_losses': episode_losses,\n",
    "        'network': agent.main_network,\n",
    "        'topology': topology,\n",
    "        'input_nodes': input_nodes,\n",
    "        'output_nodes': output_nodes,\n",
    "        'weight_history': weight_history,\n",
    "        'bias_history': bias_history,\n",
    "        'tracking_episodes': tracking_episodes\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DQN hyperparameters\n",
    "dqn_learning_rate = 0.001\n",
    "dqn_gamma = 0.99\n",
    "dqn_batch_size = 32\n",
    "num_episodes = 100\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update_freq = 10\n",
    "replay_buffer_size = 10000\n",
    "min_buffer_size = 1000\n",
    "\n",
    "# Topology parameters\n",
    "network_type = \"small_world\"\n",
    "seed = 43\n",
    "hidden_size = 32\n",
    "k = 8\n",
    "p = 0.1\n",
    "\n",
    "# Training\n",
    "print(\"=\" * 60)\n",
    "print(\"DQN Training with Custom Small-World Topology\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tracking frequency for visualization (every N episodes)\n",
    "track_freq = 10\n",
    "\n",
    "history = train_dqn(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    hidden_size=hidden_size,\n",
    "    k=k,\n",
    "    p=p,\n",
    "    seed=seed,\n",
    "    num_episodes=num_episodes,\n",
    "    batch_size=dqn_batch_size,\n",
    "    learning_rate=dqn_learning_rate,\n",
    "    gamma=dqn_gamma,\n",
    "    epsilon_start=epsilon_start,\n",
    "    epsilon_end=epsilon_end,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    target_update_freq=target_update_freq,\n",
    "    replay_buffer_size=replay_buffer_size,\n",
    "    min_buffer_size=min_buffer_size,\n",
    "    device='cpu',\n",
    "    track_freq=track_freq\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final average reward (last 50 episodes): {np.mean(history['episode_rewards'][-50:]):.2f}\")\n",
    "print(f\"Best episode reward: {max(history['episode_rewards']):.2f}\")\n",
    "\n",
    "# Generate training video\n",
    "if history.get('weight_history') and history.get('bias_history'):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Generating Training Video...\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        video_path = create_training_video(history, output_path=\"training_evolution_32_200.mp4\", fps=2)\n",
    "        print(f\"\\nVideo successfully created: {video_path}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"\\nWarning: Could not create video - {e}\")\n",
    "        print(\"Install imageio and imageio-ffmpeg to enable video generation:\")\n",
    "        print(\"  pip install imageio imageio-ffmpeg\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError creating video: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo weight/bias history available for video generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed600bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7500a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f80f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# Create environment (use \"rgb_array\" mode to render as images)\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "img = env.render()\n",
    "\n",
    "for _ in range(200):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    img = env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "    time.sleep(0.02)  # adjust for speed\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09faaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
